{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "833c2af1-821f-49ea-a5a9-2e0708397c57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.44.2)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.12/site-packages (3.0.0)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.4.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (1.4.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.24.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets torch scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c29dd017-cd38-4d87-96cb-ae326edbcd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64c0585c-4c51-4ee1-978b-6f039c407ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>\\nProject Title</th>\n",
       "      <th>Project Description</th>\n",
       "      <th>Major</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dynamics of Human Trust of AI</td>\n",
       "      <td>The purpose of this research is to investigate...</td>\n",
       "      <td>Data Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AI-Powered Avatar Generation</td>\n",
       "      <td>Research Problem : Creating lifelike avatars t...</td>\n",
       "      <td>Data Science, Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Explainable Hospital Readmission Prediction wi...</td>\n",
       "      <td>The Australian Commission on Safety and Qualit...</td>\n",
       "      <td>Data Science, Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bushfire Path prediction</td>\n",
       "      <td>Enhance the FirePath platform, built by OreFox...</td>\n",
       "      <td>Data Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Identification of Porphyry Deposits from Magne...</td>\n",
       "      <td>Research Problem \\nPorphyry deposits are signi...</td>\n",
       "      <td>Data Science</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     \\nProject Title  \\\n",
       "0                      Dynamics of Human Trust of AI   \n",
       "1                      AI-Powered Avatar Generation    \n",
       "2  Explainable Hospital Readmission Prediction wi...   \n",
       "3                           Bushfire Path prediction   \n",
       "4  Identification of Porphyry Deposits from Magne...   \n",
       "\n",
       "                                 Project Description  \\\n",
       "0  The purpose of this research is to investigate...   \n",
       "1  Research Problem : Creating lifelike avatars t...   \n",
       "2  The Australian Commission on Safety and Qualit...   \n",
       "3  Enhance the FirePath platform, built by OreFox...   \n",
       "4  Research Problem \\nPorphyry deposits are signi...   \n",
       "\n",
       "                            Major  \n",
       "0                    Data Science  \n",
       "1  Data Science, Computer Science  \n",
       "2  Data Science, Computer Science  \n",
       "3                    Data Science  \n",
       "4                    Data Science  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CSV file into a DataFrame with explicit encoding\n",
    "df = pd.read_csv('/Users/cherishkohli/new_BERT_training_dataset.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Display the first few rows to verify the data\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d8d9433-97df-473b-809f-296ce3a7b458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>\\nProject Title</th>\n",
       "      <th>Project Description</th>\n",
       "      <th>Major</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dynamics of Human Trust of AI</td>\n",
       "      <td>The purpose of this research is to investigate...</td>\n",
       "      <td>Data Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AI-Powered Avatar Generation</td>\n",
       "      <td>Research Problem : Creating lifelike avatars t...</td>\n",
       "      <td>Data Science, Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Explainable Hospital Readmission Prediction wi...</td>\n",
       "      <td>The Australian Commission on Safety and Qualit...</td>\n",
       "      <td>Data Science, Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bushfire Path prediction</td>\n",
       "      <td>Enhance the FirePath platform, built by OreFox...</td>\n",
       "      <td>Data Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Identification of Porphyry Deposits from Magne...</td>\n",
       "      <td>Research Problem  Porphyry deposits are signif...</td>\n",
       "      <td>Data Science</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     \\nProject Title  \\\n",
       "0                      Dynamics of Human Trust of AI   \n",
       "1                      AI-Powered Avatar Generation    \n",
       "2  Explainable Hospital Readmission Prediction wi...   \n",
       "3                           Bushfire Path prediction   \n",
       "4  Identification of Porphyry Deposits from Magne...   \n",
       "\n",
       "                                 Project Description  \\\n",
       "0  The purpose of this research is to investigate...   \n",
       "1  Research Problem : Creating lifelike avatars t...   \n",
       "2  The Australian Commission on Safety and Qualit...   \n",
       "3  Enhance the FirePath platform, built by OreFox...   \n",
       "4  Research Problem  Porphyry deposits are signif...   \n",
       "\n",
       "                            Major  \n",
       "0                    Data Science  \n",
       "1  Data Science, Computer Science  \n",
       "2  Data Science, Computer Science  \n",
       "3                    Data Science  \n",
       "4                    Data Science  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace newline characters with spaces in the 'Project Description' column\n",
    "df['Project Description'] = df['Project Description'].replace('\\n', ' ', regex=True)\n",
    "\n",
    "# Verify the data after cleaning\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e5612c1-c32f-4291-8a08-e0420d855632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Project Title', 'Project Description', 'Major'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Strip any leading/trailing whitespace or newlines from the column headers\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Verify the column headers to ensure they are cleaned\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f59d95dd-eeb7-406a-ad1a-ec5e21abeb35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Project Title</th>\n",
       "      <th>Project Description</th>\n",
       "      <th>Major</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dynamics of Human Trust of AI</td>\n",
       "      <td>The purpose of this research is to investigate...</td>\n",
       "      <td>Data Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AI-Powered Avatar Generation</td>\n",
       "      <td>Research Problem : Creating lifelike avatars t...</td>\n",
       "      <td>Data Science, Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Explainable Hospital Readmission Prediction wi...</td>\n",
       "      <td>The Australian Commission on Safety and Qualit...</td>\n",
       "      <td>Data Science, Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bushfire Path prediction</td>\n",
       "      <td>Enhance the FirePath platform, built by OreFox...</td>\n",
       "      <td>Data Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Identification of Porphyry Deposits from Magne...</td>\n",
       "      <td>Research Problem  Porphyry deposits are signif...</td>\n",
       "      <td>Data Science</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Project Title  \\\n",
       "0                      Dynamics of Human Trust of AI   \n",
       "1                      AI-Powered Avatar Generation    \n",
       "2  Explainable Hospital Readmission Prediction wi...   \n",
       "3                           Bushfire Path prediction   \n",
       "4  Identification of Porphyry Deposits from Magne...   \n",
       "\n",
       "                                 Project Description  \\\n",
       "0  The purpose of this research is to investigate...   \n",
       "1  Research Problem : Creating lifelike avatars t...   \n",
       "2  The Australian Commission on Safety and Qualit...   \n",
       "3  Enhance the FirePath platform, built by OreFox...   \n",
       "4  Research Problem  Porphyry deposits are signif...   \n",
       "\n",
       "                            Major  \n",
       "0                    Data Science  \n",
       "1  Data Science, Computer Science  \n",
       "2  Data Science, Computer Science  \n",
       "3                    Data Science  \n",
       "4                    Data Science  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b205d7be-dd49-4aa7-b202-5c435271f3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Project Description: The purpose of this research is to investigate the development of trust when interacting with an artificial intelligence (AI) system, to better understand  cognitive activity of humans while interacting with AI systems.     Findings of this study will improve our understanding of the various  dimensions of trust, and the ways in which trust dynamically develops in  interactions, such as human-AI systems. This will provide insights to improve  human-AI performance and efficiency in joint analytic tasks, such as  recommender systems and human-AI teaming.  As your project will contribute to an ongoing study into dynamic cognitive  trust in AI, using quantum-like models, you are expected to focus on the  following issues:  -investigate the literature on human trust in AI and its dimensions  (such as reliability and benevolence);  -work with the project supervisor to design an experiment and online  questionnaire that collects data about an aspect of human trust when  interacting with the AI system; and/or  -analysing and deriving insights from collected quantitative and  qualitative response data.  This work will contribute to research articles describing the findings of this  work within the parent-project context. The parent-project is funded by a  research grant from the US Air Force Research Lab (AFRL), which is part of the  US Department of Defence (DoD). \n",
      "Corresponding Major(s): Data Science\n"
     ]
    }
   ],
   "source": [
    "# Separate the features (project descriptions) and labels (majors)\n",
    "project_descriptions = df['Project Description'].values\n",
    "majors = df['Major'].values\n",
    "\n",
    "# Display a sample to verify\n",
    "print(f\"Sample Project Description: {project_descriptions[0]}\")\n",
    "print(f\"Corresponding Major(s): {majors[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fafe8675-7f85-412e-9b10-4c9b6c638bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First tokenized description: [101, 1996, 3800, 1997, 2023, 2470, 2003, 2000, 8556, 1996, 2458, 1997, 3404, 2043, 21935, 2007, 2019, 7976, 4454, 1006, 9932, 1007, 2291, 1010, 2000, 2488, 3305, 10699, 4023, 1997, 4286, 2096, 21935, 2007, 9932, 3001, 1012, 9556, 1997, 2023, 2817, 2097, 5335, 2256, 4824, 1997, 1996, 2536, 9646, 1997, 3404, 1010, 1998, 1996, 3971, 1999, 2029, 3404, 8790, 3973, 11791, 1999, 10266, 1010, 2107, 2004, 2529, 1011, 9932, 3001, 1012, 2023, 2097, 3073, 20062, 2000, 5335, 2529, 1011, 9932, 2836, 1998, 8122, 1999, 4101, 23521, 8518, 1010, 2107, 2004, 16755, 2121, 3001, 1998, 2529, 1011, 9932, 27025, 1012, 2004, 2115, 2622, 2097, 9002, 2000, 2019, 7552, 2817, 2046, 8790, 10699, 3404, 1999, 9932, 1010, 2478, 8559, 1011, 2066, 4275, 1010, 2017, 2024, 3517, 2000, 3579, 2006, 1996, 2206, 3314, 1024, 1011, 8556, 1996, 3906, 2006, 2529, 3404, 1999, 9932, 1998, 2049, 9646, 1006, 2107, 2004, 15258, 1998, 3841, 6777, 9890, 5897, 1007, 1025, 1011, 2147, 2007, 1996, 2622, 12366, 2000, 2640, 2019, 7551, 1998, 3784, 3160, 20589, 2008, 17427, 2951, 2055, 2019, 7814, 1997, 2529, 3404, 2043, 21935, 2007, 1996, 9932, 2291, 1025, 1998, 1013, 2030, 1011, 20302, 7274, 2075, 1998, 4315, 14966, 20062, 2013, 5067, 20155, 1998, 24209, 11475, 27453, 3433, 2951, 1012, 2023, 2147, 2097, 9002, 2000, 2470, 4790, 7851, 1996, 9556, 1997, 2023, 2147, 2306, 1996, 6687, 1011, 2622, 6123, 1012, 1996, 6687, 1011, 2622, 2003, 6787, 2011, 1037, 2470, 3946, 2013, 1996, 2149, 2250, 2486, 2470, 6845, 1006, 21358, 12190, 1007, 1010, 2029, 2003, 2112, 1997, 1996, 2149, 2533, 1997, 4721, 1006, 26489, 1007, 1012, 102]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import the BERT tokenizer from Hugging Face\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the project descriptions\n",
    "tokenized_descriptions = [tokenizer.encode(desc, add_special_tokens=True, max_length=512, truncation=True) for desc in project_descriptions]\n",
    "\n",
    "# Check the first tokenized project description\n",
    "print(f\"First tokenized description: {tokenized_descriptions[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e1d0482-a0ca-44c7-ac5a-fa2b9537a50e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First padded description: tensor([  101,  1996,  3800,  1997,  2023,  2470,  2003,  2000,  8556,  1996,\n",
      "         2458,  1997,  3404,  2043, 21935,  2007,  2019,  7976,  4454,  1006,\n",
      "         9932,  1007,  2291,  1010,  2000,  2488,  3305, 10699,  4023,  1997,\n",
      "         4286,  2096, 21935,  2007,  9932,  3001,  1012,  9556,  1997,  2023,\n",
      "         2817,  2097,  5335,  2256,  4824,  1997,  1996,  2536,  9646,  1997,\n",
      "         3404,  1010,  1998,  1996,  3971,  1999,  2029,  3404,  8790,  3973,\n",
      "        11791,  1999, 10266,  1010,  2107,  2004,  2529,  1011,  9932,  3001,\n",
      "         1012,  2023,  2097,  3073, 20062,  2000,  5335,  2529,  1011,  9932,\n",
      "         2836,  1998,  8122,  1999,  4101, 23521,  8518,  1010,  2107,  2004,\n",
      "        16755,  2121,  3001,  1998,  2529,  1011,  9932, 27025,  1012,  2004,\n",
      "         2115,  2622,  2097,  9002,  2000,  2019,  7552,  2817,  2046,  8790,\n",
      "        10699,  3404,  1999,  9932,  1010,  2478,  8559,  1011,  2066,  4275,\n",
      "         1010,  2017,  2024,  3517,  2000,  3579,  2006,  1996,  2206,  3314,\n",
      "         1024,  1011,  8556,  1996,  3906,  2006,  2529,  3404,  1999,  9932,\n",
      "         1998,  2049,  9646,  1006,  2107,  2004, 15258,  1998,  3841,  6777,\n",
      "         9890,  5897,  1007,  1025,  1011,  2147,  2007,  1996,  2622, 12366,\n",
      "         2000,  2640,  2019,  7551,  1998,  3784,  3160, 20589,  2008, 17427,\n",
      "         2951,  2055,  2019,  7814,  1997,  2529,  3404,  2043, 21935,  2007,\n",
      "         1996,  9932,  2291,  1025,  1998,  1013,  2030,  1011, 20302,  7274,\n",
      "         2075,  1998,  4315, 14966, 20062,  2013,  5067, 20155,  1998, 24209,\n",
      "        11475, 27453,  3433,  2951,  1012,  2023,  2147,  2097,  9002,  2000,\n",
      "         2470,  4790,  7851,  1996,  9556,  1997,  2023,  2147,  2306,  1996,\n",
      "         6687,  1011,  2622,  6123,  1012,  1996,  6687,  1011,  2622,  2003,\n",
      "         6787,  2011,  1037,  2470,  3946,  2013,  1996,  2149,  2250,  2486,\n",
      "         2470,  6845,  1006, 21358, 12190,  1007,  1010,  2029,  2003,  2112,\n",
      "         1997,  1996,  2149,  2533,  1997,  4721,  1006, 26489,  1007,  1012,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Pad all tokenized sequences to the same length (BERT's maximum length is 512 tokens)\n",
    "MAX_LEN = 512\n",
    "padded_descriptions = torch.nn.utils.rnn.pad_sequence([torch.tensor(t) for t in tokenized_descriptions], \n",
    "                                                      batch_first=True, \n",
    "                                                      padding_value=0)\n",
    "\n",
    "# Check the first padded sequence\n",
    "print(f\"First padded description: {padded_descriptions[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb53f8d9-ac0f-4ccb-a2a4-8018d22a526f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First project binary labels: [0 0 1 0]\n",
      "Classes: ['Computer Science' 'Cyber Security' 'Data Science' 'Software Development']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Convert the \"Major\" column into a list of lists (since some projects have multiple majors)\n",
    "majors = df['Major'].apply(lambda x: x.split(', '))\n",
    "\n",
    "# Use MultiLabelBinarizer to encode the majors into binary labels\n",
    "mlb = MultiLabelBinarizer()\n",
    "binary_labels = mlb.fit_transform(majors)\n",
    "\n",
    "# Print the binary labels for the first project\n",
    "print(f\"First project binary labels: {binary_labels[0]}\")\n",
    "print(f\"Classes: {mlb.classes_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75f5c4a6-6c48-47d4-9b45-1508298838aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First input id set: tensor([  101,  1996,  3800,  1997,  2023,  2470,  2003,  2000,  8556,  1996,\n",
      "         2458,  1997,  3404,  2043, 21935,  2007,  2019,  7976,  4454,  1006,\n",
      "         9932,  1007,  2291,  1010,  2000,  2488,  3305, 10699,  4023,  1997,\n",
      "         4286,  2096, 21935,  2007,  9932,  3001,  1012,  9556,  1997,  2023,\n",
      "         2817,  2097,  5335,  2256,  4824,  1997,  1996,  2536,  9646,  1997,\n",
      "         3404,  1010,  1998,  1996,  3971,  1999,  2029,  3404,  8790,  3973,\n",
      "        11791,  1999, 10266,  1010,  2107,  2004,  2529,  1011,  9932,  3001,\n",
      "         1012,  2023,  2097,  3073, 20062,  2000,  5335,  2529,  1011,  9932,\n",
      "         2836,  1998,  8122,  1999,  4101, 23521,  8518,  1010,  2107,  2004,\n",
      "        16755,  2121,  3001,  1998,  2529,  1011,  9932, 27025,  1012,  2004,\n",
      "         2115,  2622,  2097,  9002,  2000,  2019,  7552,  2817,  2046,  8790,\n",
      "        10699,  3404,  1999,  9932,  1010,  2478,  8559,  1011,  2066,  4275,\n",
      "         1010,  2017,  2024,  3517,  2000,  3579,  2006,  1996,  2206,  3314,\n",
      "         1024,  1011,  8556,  1996,  3906,  2006,  2529,  3404,  1999,  9932,\n",
      "         1998,  2049,  9646,  1006,  2107,  2004, 15258,  1998,  3841,  6777,\n",
      "         9890,  5897,  1007,  1025,  1011,  2147,  2007,  1996,  2622, 12366,\n",
      "         2000,  2640,  2019,  7551,  1998,  3784,  3160, 20589,  2008, 17427,\n",
      "         2951,  2055,  2019,  7814,  1997,  2529,  3404,  2043, 21935,  2007,\n",
      "         1996,  9932,  2291,  1025,  1998,  1013,  2030,  1011, 20302,  7274,\n",
      "         2075,  1998,  4315, 14966, 20062,  2013,  5067, 20155,  1998, 24209,\n",
      "        11475, 27453,  3433,  2951,  1012,  2023,  2147,  2097,  9002,  2000,\n",
      "         2470,  4790,  7851,  1996,  9556,  1997,  2023,  2147,  2306,  1996,\n",
      "         6687,  1011,  2622,  6123,  1012,  1996,  6687,  1011,  2622,  2003,\n",
      "         6787,  2011,  1037,  2470,  3946,  2013,  1996,  2149,  2250,  2486,\n",
      "         2470,  6845,  1006, 21358, 12190,  1007,  1010,  2029,  2003,  2112,\n",
      "         1997,  1996,  2149,  2533,  1997,  4721,  1006, 26489,  1007,  1012,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0])\n"
     ]
    }
   ],
   "source": [
    "# Define input_ids from padded_descriptions (it's already a tensor)\n",
    "input_ids = padded_descriptions\n",
    "\n",
    "# Ensure the input_ids have been properly set\n",
    "print(f\"First input id set: {input_ids[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac83c11b-bbe5-4e2c-bdbe-e572a9d8b340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 25\n",
      "Validation set size: 7\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and validation sets (80% training, 20% validation)\n",
    "train_inputs, val_inputs, train_labels, val_labels = train_test_split(\n",
    "    input_ids, binary_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Print the size of the training and validation sets\n",
    "print(f\"Training set size: {len(train_inputs)}\")\n",
    "print(f\"Validation set size: {len(val_inputs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff9fed49-0a83-4b46-a8bb-102b91975f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First training mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Create attention masks: 1 for real tokens, 0 for padding tokens\n",
    "attention_masks = [[int(token_id > 0) for token_id in input_id] for input_id in input_ids]\n",
    "\n",
    "# Split the attention masks as well into training and validation sets\n",
    "train_masks, val_masks = train_test_split(attention_masks, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the first training mask to verify\n",
    "print(f\"First training mask: {train_masks[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c67c4f5-25be-444d-ab56-c8bde270cd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training input tensor shape: torch.Size([25, 512])\n",
      "Validation input tensor shape: torch.Size([7, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_0/0g3f756n2wdcj_w28pcyfpw80000gn/T/ipykernel_64580/4016253900.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_inputs = torch.tensor(train_inputs)\n",
      "/var/folders/_0/0g3f756n2wdcj_w28pcyfpw80000gn/T/ipykernel_64580/4016253900.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_inputs = torch.tensor(val_inputs)\n"
     ]
    }
   ],
   "source": [
    "# Convert all inputs, labels, and masks into torch tensors\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "\n",
    "# Check the tensor sizes\n",
    "print(f\"Training input tensor shape: {train_inputs.shape}\")\n",
    "print(f\"Validation input tensor shape: {val_inputs.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d68359b-e0be-4c82-95ae-670f5c01b6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 101, 2195, 2330,  ...,    0,    0,    0],\n",
      "        [ 101, 2023, 2470,  ...,    0,    0,    0],\n",
      "        [ 101, 2470, 3291,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 3151, 3274,  ...,    0,    0,    0],\n",
      "        [ 101, 2470, 3291,  ...,    0,    0,    0],\n",
      "        [ 101, 2715, 4773,  ...,    0,    0,    0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), tensor([[0, 1, 0, 0],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [1, 0, 0, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [1, 0, 0, 0],\n",
      "        [0, 1, 0, 0]])]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "\n",
    "# Combine the training inputs, masks, and labels into a TensorDataset\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "\n",
    "# Use DataLoader to handle batches of data during training\n",
    "batch_size = 8\n",
    "\n",
    "# Create DataLoaders for the training and validation datasets\n",
    "train_dataloader = DataLoader(\n",
    "    train_data,  # Training data\n",
    "    sampler=RandomSampler(train_data),  # Random sampling for training\n",
    "    batch_size=batch_size  # Batch size\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_data,  # Validation data\n",
    "    sampler=SequentialSampler(val_data),  # Sequential sampling for validation\n",
    "    batch_size=batch_size  # Batch size\n",
    ")\n",
    "\n",
    "# Print the first batch of training data to verify\n",
    "for batch in train_dataloader:\n",
    "    print(batch)\n",
    "    break  # Just print the first batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d36061bb-251d-4b15-8e53-47f965fccdb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First training example: {'input_ids': tensor([  101,  2195,  2330,  1011,  3120,  3934,  3298,  2715,  1011,  2154,\n",
      "         2009,  5097,  1012,  2174,  1010,  2070,  2330,  1011,  3120,  3934,\n",
      "         2131, 20419,  2011, 24391, 17857,  1010,  2040,  2421, 15451,  8059,\n",
      "         2000,  1996,  3642,  2000, 12014,  1996,  3036,  1997,  1996,  4646,\n",
      "         5198,  1012,  2023,  2622,  2097,  8556,  8107,  2005, 12329,  1996,\n",
      "         2330,  1011,  3120,  4007,  1012,  2057,  2031,  2019,  1999,  1011,\n",
      "         2160,  6994,  2000, 20302, 23274, 21025,  2705, 12083,  2330,  1011,\n",
      "         3120,  4007,  1012,  2017,  2024,  3517,  2000,  3579,  2006,  1996,\n",
      "         2206,  3314,  1024,  1011,  8556,  1996,  3906,  2006,  2330,  1011,\n",
      "         3120,  4007,  1998,  2049,  3036,  1011,  2147,  2007,  1996,  2622,\n",
      "        12366,  2000,  4503,  1037,  2640,  2005, 12329,  1996,  2330,  1011,\n",
      "         3120,  4007,  1012,  2057,  2933,  2000,  4339,  1037,  2470,  3720,\n",
      "         7851,  1996,  9556,  1997,  2023,  2147,  1012,  2023,  2622,  2038,\n",
      "        14269,  4022,  2005,  2119,  1996,  3068,  1998,  1996,  2470,  2451,\n",
      "         2065,  1996,  2470,  3450,  2024,  3344,  2041, 29454, 29206, 14626,\n",
      "         1012,  2057,  2097,  2713,  2019,  2330,  1011,  3120,  4007,  2247,\n",
      "         2007,  1037,  2470,  3259,   102,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([0, 1, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "# Function to convert inputs and labels into a format Hugging Face Trainer expects\n",
    "def convert_to_dataset(input_ids, attention_masks, labels):\n",
    "    return [{'input_ids': input_id, 'attention_mask': attention_mask, 'labels': label}\n",
    "            for input_id, attention_mask, label in zip(input_ids, attention_masks, labels)]\n",
    "\n",
    "# Convert the training and validation data to the expected format\n",
    "train_data = convert_to_dataset(train_inputs, train_masks, train_labels)\n",
    "val_data = convert_to_dataset(val_inputs, val_masks, val_labels)\n",
    "\n",
    "# Verify the format of the training data\n",
    "print(f\"First training example: {train_data[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36e21ed5-e5b0-46c3-80fe-33f8ce45e905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the model is set for multi-label classification\n",
    "model.config.problem_type = \"multi_label_classification\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c948050-e9d9-4601-b4b9-940469bd4357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-label classification using BCEWithLogitsLoss\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "\n",
    "def compute_loss(model, inputs):\n",
    "    labels = inputs.pop(\"labels\")\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    loss_fct = BCEWithLogitsLoss()\n",
    "    loss = loss_fct(logits, labels.float())\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f778cf92-9961-49c8-b57c-a63832e14beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12, training_loss=0.6879624327023824, metrics={'train_runtime': 18.6871, 'train_samples_per_second': 4.013, 'train_steps_per_second': 0.642, 'total_flos': 19733683507200.0, 'train_loss': 0.6879624327023824, 'epoch': 3.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "\n",
    "# Custom Trainer with overridden compute_loss for multi-label classification\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fct = BCEWithLogitsLoss()\n",
    "        loss = loss_fct(logits, labels.float())  # Make sure labels are float for BCEWithLogitsLoss\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Initialize the CustomTrainer with the correct parameters\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,  # Add the data collator\n",
    ")\n",
    "\n",
    "# Start the training process\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6119af0-6616-4788-a346-da604035b117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted majors: [('Data Science', 'Software Development')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if MPS (Apple Silicon) is available, otherwise use CPU\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "model.to(device)\n",
    "\n",
    "# Example project description with triple quotes for multi-line string\n",
    "project_description = \"\"\"\n",
    "Research Problem\n",
    "Porphyry deposits are significant sources of copper, gold, and other metals, \n",
    "and their identification is crucial for mineral exploration. Traditional methods\n",
    "of detecting these deposits from magnetic data are often labor-intensive and\n",
    "subject to human error. This research aims to leverage object detection\n",
    "techniques to automate and improve the accuracy of identifying porphyry\n",
    "deposits from magnetic property images.\n",
    "\n",
    "Aims\n",
    "1. Develop a Methodology: Establish a systematic approach for\n",
    "analyzing magnetic property images to identify porphyry deposits.\n",
    "2. Implement Object Detection Algorithms: Apply and optimize object\n",
    "detection models to recognize geological features indicative of\n",
    "porphyry deposits.\n",
    "3. Evaluate Model Performance: Assess the accuracy and efficiency of\n",
    "different object detection algorithms in identifying porphyry deposits.\n",
    "\n",
    "Method\n",
    "1. Understanding Magnetic Properties: Identify magnetic\n",
    "characteristics associated with porphyry deposits and study their\n",
    "regional variations.\n",
    "2. Data Acquisition and Preprocessing: Source high-quality magnetic\n",
    "property images and perform necessary preprocessing to clean and\n",
    "enhance these images for analysis.\n",
    "3. Feature Extraction: Extract and quantify key features or patterns in\n",
    "the magnetic property images that indicate the presence of porphyry\n",
    "deposits.\n",
    "4. Object Detection Algorithms: Test and optimize object detection\n",
    "algorithms (e.g., YOLO, Faster R-CNN) for identifying geological\n",
    "features.\n",
    "5. Model Training and Validation: Split data into training, validation,\n",
    "and test sets, and ensure robust model training and validation.\n",
    "6. Model Performance Evaluation: Use metrics such as precision, recall,\n",
    "F1 score, and Intersection over Union to evaluate model\n",
    "performance.\n",
    "7. Challenges and Limitations: Identify main challenges and address\n",
    "them through methodological or technological improvements.\n",
    "8. Practical Applications: Integrate the developed methodology into\n",
    "existing geological exploration workflows and highlight potential\n",
    "benefits.\n",
    "\n",
    "Expected Outputs\n",
    "1. A robust methodology for analyzing magnetic property images to\n",
    "identify porphyry deposits.\n",
    "2. Optimized object detection algorithms tailored for geological feature\n",
    "identification.\n",
    "3. Comprehensive performance evaluation of various object detection\n",
    "models.\n",
    "4. Practical guidelines for integrating the methodology into geological\n",
    "exploration workflows.\n",
    "5. Identification of challenges and proposed solutions for future\n",
    "research directions.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize and prepare input\n",
    "inputs = tokenizer(project_description, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Move inputs to the correct device\n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "# Model prediction\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "\n",
    "# Apply sigmoid to convert logits to probabilities\n",
    "probabilities = torch.sigmoid(logits)\n",
    "\n",
    "# Define threshold (0.5 is common)\n",
    "predicted_labels = (probabilities > 0.5).int()\n",
    "\n",
    "# Print predicted majors\n",
    "predicted_majors = mlb.inverse_transform(predicted_labels.detach().cpu().numpy())  # Move to CPU if necessary\n",
    "print(f\"Predicted majors: {predicted_majors}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e8828c-76d2-4189-942d-e1e82e3263b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
